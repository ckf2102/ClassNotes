{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Naive Bayes and Text Data#\n",
    "### 15 July 2015 ###\n",
    "\n",
    "**Applying Bayes Theorem to the Iris Data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the iris data\n",
    "iris = load_iris()\n",
    "\n",
    "# round up the measurements\n",
    "X = np.ceil(iris.data)\n",
    "\n",
    "# clean up column names\n",
    "col_names = [name[:-5].replace(' ', '_') for name in iris.feature_names]\n",
    "\n",
    "# read into pandas\n",
    "df = pd.DataFrame(X, columns=col_names)\n",
    "\n",
    "# create a list of species using iris.target and iris.target_names\n",
    "species = [iris.target_names[num] for num in iris.target]\n",
    "\n",
    "# add the species list as a new DataFrame column\n",
    "df['species'] = species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DESCR': 'Iris Plants Database\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n',\n",
       " 'data': array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "        [ 4.9,  3. ,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.3,  0.2],\n",
       "        [ 4.6,  3.1,  1.5,  0.2],\n",
       "        [ 5. ,  3.6,  1.4,  0.2],\n",
       "        [ 5.4,  3.9,  1.7,  0.4],\n",
       "        [ 4.6,  3.4,  1.4,  0.3],\n",
       "        [ 5. ,  3.4,  1.5,  0.2],\n",
       "        [ 4.4,  2.9,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5.4,  3.7,  1.5,  0.2],\n",
       "        [ 4.8,  3.4,  1.6,  0.2],\n",
       "        [ 4.8,  3. ,  1.4,  0.1],\n",
       "        [ 4.3,  3. ,  1.1,  0.1],\n",
       "        [ 5.8,  4. ,  1.2,  0.2],\n",
       "        [ 5.7,  4.4,  1.5,  0.4],\n",
       "        [ 5.4,  3.9,  1.3,  0.4],\n",
       "        [ 5.1,  3.5,  1.4,  0.3],\n",
       "        [ 5.7,  3.8,  1.7,  0.3],\n",
       "        [ 5.1,  3.8,  1.5,  0.3],\n",
       "        [ 5.4,  3.4,  1.7,  0.2],\n",
       "        [ 5.1,  3.7,  1.5,  0.4],\n",
       "        [ 4.6,  3.6,  1. ,  0.2],\n",
       "        [ 5.1,  3.3,  1.7,  0.5],\n",
       "        [ 4.8,  3.4,  1.9,  0.2],\n",
       "        [ 5. ,  3. ,  1.6,  0.2],\n",
       "        [ 5. ,  3.4,  1.6,  0.4],\n",
       "        [ 5.2,  3.5,  1.5,  0.2],\n",
       "        [ 5.2,  3.4,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.6,  0.2],\n",
       "        [ 4.8,  3.1,  1.6,  0.2],\n",
       "        [ 5.4,  3.4,  1.5,  0.4],\n",
       "        [ 5.2,  4.1,  1.5,  0.1],\n",
       "        [ 5.5,  4.2,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5. ,  3.2,  1.2,  0.2],\n",
       "        [ 5.5,  3.5,  1.3,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 4.4,  3. ,  1.3,  0.2],\n",
       "        [ 5.1,  3.4,  1.5,  0.2],\n",
       "        [ 5. ,  3.5,  1.3,  0.3],\n",
       "        [ 4.5,  2.3,  1.3,  0.3],\n",
       "        [ 4.4,  3.2,  1.3,  0.2],\n",
       "        [ 5. ,  3.5,  1.6,  0.6],\n",
       "        [ 5.1,  3.8,  1.9,  0.4],\n",
       "        [ 4.8,  3. ,  1.4,  0.3],\n",
       "        [ 5.1,  3.8,  1.6,  0.2],\n",
       "        [ 4.6,  3.2,  1.4,  0.2],\n",
       "        [ 5.3,  3.7,  1.5,  0.2],\n",
       "        [ 5. ,  3.3,  1.4,  0.2],\n",
       "        [ 7. ,  3.2,  4.7,  1.4],\n",
       "        [ 6.4,  3.2,  4.5,  1.5],\n",
       "        [ 6.9,  3.1,  4.9,  1.5],\n",
       "        [ 5.5,  2.3,  4. ,  1.3],\n",
       "        [ 6.5,  2.8,  4.6,  1.5],\n",
       "        [ 5.7,  2.8,  4.5,  1.3],\n",
       "        [ 6.3,  3.3,  4.7,  1.6],\n",
       "        [ 4.9,  2.4,  3.3,  1. ],\n",
       "        [ 6.6,  2.9,  4.6,  1.3],\n",
       "        [ 5.2,  2.7,  3.9,  1.4],\n",
       "        [ 5. ,  2. ,  3.5,  1. ],\n",
       "        [ 5.9,  3. ,  4.2,  1.5],\n",
       "        [ 6. ,  2.2,  4. ,  1. ],\n",
       "        [ 6.1,  2.9,  4.7,  1.4],\n",
       "        [ 5.6,  2.9,  3.6,  1.3],\n",
       "        [ 6.7,  3.1,  4.4,  1.4],\n",
       "        [ 5.6,  3. ,  4.5,  1.5],\n",
       "        [ 5.8,  2.7,  4.1,  1. ],\n",
       "        [ 6.2,  2.2,  4.5,  1.5],\n",
       "        [ 5.6,  2.5,  3.9,  1.1],\n",
       "        [ 5.9,  3.2,  4.8,  1.8],\n",
       "        [ 6.1,  2.8,  4. ,  1.3],\n",
       "        [ 6.3,  2.5,  4.9,  1.5],\n",
       "        [ 6.1,  2.8,  4.7,  1.2],\n",
       "        [ 6.4,  2.9,  4.3,  1.3],\n",
       "        [ 6.6,  3. ,  4.4,  1.4],\n",
       "        [ 6.8,  2.8,  4.8,  1.4],\n",
       "        [ 6.7,  3. ,  5. ,  1.7],\n",
       "        [ 6. ,  2.9,  4.5,  1.5],\n",
       "        [ 5.7,  2.6,  3.5,  1. ],\n",
       "        [ 5.5,  2.4,  3.8,  1.1],\n",
       "        [ 5.5,  2.4,  3.7,  1. ],\n",
       "        [ 5.8,  2.7,  3.9,  1.2],\n",
       "        [ 6. ,  2.7,  5.1,  1.6],\n",
       "        [ 5.4,  3. ,  4.5,  1.5],\n",
       "        [ 6. ,  3.4,  4.5,  1.6],\n",
       "        [ 6.7,  3.1,  4.7,  1.5],\n",
       "        [ 6.3,  2.3,  4.4,  1.3],\n",
       "        [ 5.6,  3. ,  4.1,  1.3],\n",
       "        [ 5.5,  2.5,  4. ,  1.3],\n",
       "        [ 5.5,  2.6,  4.4,  1.2],\n",
       "        [ 6.1,  3. ,  4.6,  1.4],\n",
       "        [ 5.8,  2.6,  4. ,  1.2],\n",
       "        [ 5. ,  2.3,  3.3,  1. ],\n",
       "        [ 5.6,  2.7,  4.2,  1.3],\n",
       "        [ 5.7,  3. ,  4.2,  1.2],\n",
       "        [ 5.7,  2.9,  4.2,  1.3],\n",
       "        [ 6.2,  2.9,  4.3,  1.3],\n",
       "        [ 5.1,  2.5,  3. ,  1.1],\n",
       "        [ 5.7,  2.8,  4.1,  1.3],\n",
       "        [ 6.3,  3.3,  6. ,  2.5],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 7.1,  3. ,  5.9,  2.1],\n",
       "        [ 6.3,  2.9,  5.6,  1.8],\n",
       "        [ 6.5,  3. ,  5.8,  2.2],\n",
       "        [ 7.6,  3. ,  6.6,  2.1],\n",
       "        [ 4.9,  2.5,  4.5,  1.7],\n",
       "        [ 7.3,  2.9,  6.3,  1.8],\n",
       "        [ 6.7,  2.5,  5.8,  1.8],\n",
       "        [ 7.2,  3.6,  6.1,  2.5],\n",
       "        [ 6.5,  3.2,  5.1,  2. ],\n",
       "        [ 6.4,  2.7,  5.3,  1.9],\n",
       "        [ 6.8,  3. ,  5.5,  2.1],\n",
       "        [ 5.7,  2.5,  5. ,  2. ],\n",
       "        [ 5.8,  2.8,  5.1,  2.4],\n",
       "        [ 6.4,  3.2,  5.3,  2.3],\n",
       "        [ 6.5,  3. ,  5.5,  1.8],\n",
       "        [ 7.7,  3.8,  6.7,  2.2],\n",
       "        [ 7.7,  2.6,  6.9,  2.3],\n",
       "        [ 6. ,  2.2,  5. ,  1.5],\n",
       "        [ 6.9,  3.2,  5.7,  2.3],\n",
       "        [ 5.6,  2.8,  4.9,  2. ],\n",
       "        [ 7.7,  2.8,  6.7,  2. ],\n",
       "        [ 6.3,  2.7,  4.9,  1.8],\n",
       "        [ 6.7,  3.3,  5.7,  2.1],\n",
       "        [ 7.2,  3.2,  6. ,  1.8],\n",
       "        [ 6.2,  2.8,  4.8,  1.8],\n",
       "        [ 6.1,  3. ,  4.9,  1.8],\n",
       "        [ 6.4,  2.8,  5.6,  2.1],\n",
       "        [ 7.2,  3. ,  5.8,  1.6],\n",
       "        [ 7.4,  2.8,  6.1,  1.9],\n",
       "        [ 7.9,  3.8,  6.4,  2. ],\n",
       "        [ 6.4,  2.8,  5.6,  2.2],\n",
       "        [ 6.3,  2.8,  5.1,  1.5],\n",
       "        [ 6.1,  2.6,  5.6,  1.4],\n",
       "        [ 7.7,  3. ,  6.1,  2.3],\n",
       "        [ 6.3,  3.4,  5.6,  2.4],\n",
       "        [ 6.4,  3.1,  5.5,  1.8],\n",
       "        [ 6. ,  3. ,  4.8,  1.8],\n",
       "        [ 6.9,  3.1,  5.4,  2.1],\n",
       "        [ 6.7,  3.1,  5.6,  2.4],\n",
       "        [ 6.9,  3.1,  5.1,  2.3],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 6.8,  3.2,  5.9,  2.3],\n",
       "        [ 6.7,  3.3,  5.7,  2.5],\n",
       "        [ 6.7,  3. ,  5.2,  2.3],\n",
       "        [ 6.3,  2.5,  5. ,  1.9],\n",
       "        [ 6.5,  3. ,  5.2,  2. ],\n",
       "        [ 6.2,  3.4,  5.4,  2.3],\n",
       "        [ 5.9,  3. ,  5.1,  1.8]]),\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], \n",
       "       dtype='|S10')}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0             6            4             2            1  setosa\n",
       "1             5            3             2            1  setosa\n",
       "2             5            4             2            1  setosa\n",
       "3             5            4             2            1  setosa\n",
       "4             5            4             2            1  setosa"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say there's an out of sample observation of 7, 3, 5, 2\n",
    "\n",
    "Predict the species of iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width     species\n",
       "54              7            3             5            2  versicolor\n",
       "58              7            3             5            2  versicolor\n",
       "63              7            3             5            2  versicolor\n",
       "68              7            3             5            2  versicolor\n",
       "72              7            3             5            2  versicolor\n",
       "73              7            3             5            2  versicolor\n",
       "74              7            3             5            2  versicolor\n",
       "75              7            3             5            2  versicolor\n",
       "76              7            3             5            2  versicolor\n",
       "77              7            3             5            2  versicolor\n",
       "87              7            3             5            2  versicolor\n",
       "91              7            3             5            2  versicolor\n",
       "97              7            3             5            2  versicolor\n",
       "123             7            3             5            2   virginica\n",
       "126             7            3             5            2   virginica\n",
       "127             7            3             5            2   virginica\n",
       "146             7            3             5            2   virginica"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look for all observations with features [7, 3, 5, 2]\n",
    "df[(df.sepal_length==7) & (df.sepal_width==3) & (df.petal_length == 5) & (df.petal_width==2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "versicolor    13\n",
       "virginica      4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the species for the observations\n",
    "df[(df.sepal_length==7) & (df.sepal_width==3) & (df.petal_length == 5) & (df.petal_width==2)].species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "setosa        50\n",
       "versicolor    50\n",
       "virginica     50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count species for all observations\n",
    "df.species.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So looking at it with Baye's theorem... calculate the probability of each class given the measurements\n",
    "\n",
    "$$P(versicolor|7352) = P(7352|versicolor) * P(versicolor) \n",
    "/ P(7352)$$\n",
    "\n",
    "$$P(versicolor|7352) = (13/50) * (50/150) / (17/150)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We frame a classification problem as three different conditional probability equations, using Bayes theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Naive Bayes Classification##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read tab-separated file using pandas\n",
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT7/master/data/sms.tsv'\n",
    "sms = pd.read_table(url, sep='\\t', header=None, names=['label', 'msg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms['label'] = sms.label.map({'ham':0, 'spam':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                                msg\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1393L,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sms.msg, sms.label, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179L,)\n",
      "(1393L,)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Count Vectorizer**###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of three sample text messages (pretend data)\n",
    "train_simple = ['call you tonight',\n",
    "                'Call me a cab',\n",
    "                'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
       "        charset_error=None, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count Vectorizer is not a model but it has a fit method\n",
    "vect.fit(train_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'cab', u'call', u'me', u'please', u'tonight', u'you']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns:\n",
    "- lower case\n",
    "- unique words\n",
    "- ignores 'a'\n",
    "- no punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_simple_dtm = vect.transform(train_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Transform* --> transform to *document term matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_simple_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 1, 1],\n",
       "       [1, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 2, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_simple_dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 x 6 matrix\n",
    "- one row for each message\n",
    "- one column for each token (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_simple_dtm.toarray(), columns = vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating test data (pretend)\n",
    "test_simple = [\"please don't call me\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_simple_dtm = vect.transform(test_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_simple_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   1       1        0    0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_simple_dtm.toarray(), columns = vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It ignores words that it has never seen before. Why?\n",
    "\n",
    "1. We can't do anything in Naive Bayes with wordss we've never seen before (\"don't\")\n",
    "2. We're training the model on six features (the six individual words). Can't test on a different number of features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the steps for using vectorizer:\n",
    "1. Fit --> learn vocabulary\n",
    "    - Fit on the training data\n",
    "    - Transform on the training data\n",
    "2. Transform --> take the vocabulary we have used to construct the document term matrix\n",
    "    - Transform on the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Now using on the main sms data###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to re-instantiate\n",
    "vect = CountVectorizer()\n",
    "train_dtm = vect.fit_transform(X_train)\n",
    "    # since we have to do a fit and then a transform on the training data, there's a neat function that combines the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x7456 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 55209 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1393x7456 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 17604 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'00', u'000', u'008704050406', u'0121', u'01223585236', u'01223585334', u'0125698789', u'02', u'0207', u'02072069400', u'02073162414', u'02085076972', u'021', u'03', u'04', u'0430', u'05', u'050703', u'0578', u'06', u'07', u'07008009200', u'07090201529', u'07090298926', u'07123456789', u'07732584351', u'07734396839', u'07742676969', u'0776xxxxxxx', u'07781482378', u'07786200117', u'078', u'07801543489', u'07808', u'07808247860', u'07808726822', u'07815296484', u'07821230901', u'07880867867', u'0789xxxxxxx', u'07946746291', u'0796xxxxxx', u'07973788240', u'07xxxxxxxxx', u'08', u'0800', u'08000407165', u'08000776320', u'08000839402', u'08000930705']\n",
      "[u'yer', u'yes', u'yest', u'yesterday', u'yet', u'yetunde', u'yijue', u'ym', u'ymca', u'yo', u'yoga', u'yogasana', u'yor', u'yorge', u'you', u'youdoing', u'youi', u'youphone', u'your', u'youre', u'yourjob', u'yours', u'yourself', u'youwanna', u'yowifes', u'yoyyooo', u'yr', u'yrs', u'ything', u'yummmm', u'yummy', u'yun', u'yunny', u'yuo', u'yuou', u'yup', u'zac', u'zaher', u'zealand', u'zebra', u'zed', u'zeros', u'zhong', u'zindgi', u'zoe', u'zoom', u'zouk', u'zyada', u'\\xe8n', u'\\u3028ud']\n"
     ]
    }
   ],
   "source": [
    "## EXAMINING THE FEATURES AND THEIR COUNTS\n",
    "\n",
    "# store feature names and examine them\n",
    "train_features = vect.get_feature_names()\n",
    "print train_features[:50]\n",
    "print train_features[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert train_dtm to a regular array\n",
    "train_arr = train_dtm.toarray()\n",
    "train_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 23,  2, ...,  1,  1,  1], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count how many times EACH token appears across ALL messages in train_arr\n",
    "# sum over an axis\n",
    "import numpy as np\n",
    "np.sum(train_arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>1</td>\n",
       "      <td>jules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>1</td>\n",
       "      <td>mallika</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4169</th>\n",
       "      <td>1</td>\n",
       "      <td>malarky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4165</th>\n",
       "      <td>1</td>\n",
       "      <td>makiing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4161</th>\n",
       "      <td>1</td>\n",
       "      <td>maintaining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4158</th>\n",
       "      <td>1</td>\n",
       "      <td>mails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4157</th>\n",
       "      <td>1</td>\n",
       "      <td>mailed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4151</th>\n",
       "      <td>1</td>\n",
       "      <td>magicalsongs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4150</th>\n",
       "      <td>1</td>\n",
       "      <td>maggi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4149</th>\n",
       "      <td>1</td>\n",
       "      <td>magazine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4146</th>\n",
       "      <td>1</td>\n",
       "      <td>madodu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4143</th>\n",
       "      <td>1</td>\n",
       "      <td>mad2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4142</th>\n",
       "      <td>1</td>\n",
       "      <td>mad1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4140</th>\n",
       "      <td>1</td>\n",
       "      <td>macs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4139</th>\n",
       "      <td>1</td>\n",
       "      <td>macleran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>1</td>\n",
       "      <td>mack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>1</td>\n",
       "      <td>manage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>1</td>\n",
       "      <td>manageable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4178</th>\n",
       "      <td>1</td>\n",
       "      <td>manchester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4179</th>\n",
       "      <td>1</td>\n",
       "      <td>manda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4201</th>\n",
       "      <td>1</td>\n",
       "      <td>marking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>1</td>\n",
       "      <td>marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4197</th>\n",
       "      <td>1</td>\n",
       "      <td>marine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4196</th>\n",
       "      <td>1</td>\n",
       "      <td>margin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4193</th>\n",
       "      <td>1</td>\n",
       "      <td>marandratha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192</th>\n",
       "      <td>1</td>\n",
       "      <td>maraikara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4191</th>\n",
       "      <td>1</td>\n",
       "      <td>maps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4136</th>\n",
       "      <td>1</td>\n",
       "      <td>machi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4190</th>\n",
       "      <td>1</td>\n",
       "      <td>mapquest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4187</th>\n",
       "      <td>1</td>\n",
       "      <td>manual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>292</td>\n",
       "      <td>do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7257</th>\n",
       "      <td>293</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7120</th>\n",
       "      <td>293</td>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6904</th>\n",
       "      <td>297</td>\n",
       "      <td>ur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>298</td>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>299</td>\n",
       "      <td>get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>302</td>\n",
       "      <td>if</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4778</th>\n",
       "      <td>306</td>\n",
       "      <td>or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>332</td>\n",
       "      <td>but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4647</th>\n",
       "      <td>338</td>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6017</th>\n",
       "      <td>344</td>\n",
       "      <td>so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>349</td>\n",
       "      <td>can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>358</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4662</th>\n",
       "      <td>361</td>\n",
       "      <td>now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4743</th>\n",
       "      <td>390</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3235</th>\n",
       "      <td>416</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552</th>\n",
       "      <td>443</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6539</th>\n",
       "      <td>453</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>460</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7424</th>\n",
       "      <td>508</td>\n",
       "      <td>your</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2821</th>\n",
       "      <td>518</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>550</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3623</th>\n",
       "      <td>568</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4238</th>\n",
       "      <td>601</td>\n",
       "      <td>me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3612</th>\n",
       "      <td>679</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3502</th>\n",
       "      <td>683</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>717</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6542</th>\n",
       "      <td>1004</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7420</th>\n",
       "      <td>1660</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6656</th>\n",
       "      <td>1670</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7456 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      count         token\n",
       "3727      1         jules\n",
       "4172      1       mallika\n",
       "4169      1       malarky\n",
       "4165      1       makiing\n",
       "4161      1   maintaining\n",
       "4158      1         mails\n",
       "4157      1        mailed\n",
       "4151      1  magicalsongs\n",
       "4150      1         maggi\n",
       "4149      1      magazine\n",
       "4146      1        madodu\n",
       "4143      1          mad2\n",
       "4142      1          mad1\n",
       "4140      1          macs\n",
       "4139      1      macleran\n",
       "4138      1          mack\n",
       "4174      1        manage\n",
       "4175      1    manageable\n",
       "4178      1    manchester\n",
       "4179      1         manda\n",
       "4201      1       marking\n",
       "4200      1     marketing\n",
       "4197      1        marine\n",
       "4196      1        margin\n",
       "4193      1   marandratha\n",
       "4192      1     maraikara\n",
       "4191      1          maps\n",
       "4136      1         machi\n",
       "4190      1      mapquest\n",
       "4187      1        manual\n",
       "...     ...           ...\n",
       "2290    292            do\n",
       "7257    293          with\n",
       "7120    293            we\n",
       "6904    297            ur\n",
       "1081    298            at\n",
       "2995    299           get\n",
       "3465    302            if\n",
       "4778    306            or\n",
       "1522    332           but\n",
       "4647    338           not\n",
       "6017    344            so\n",
       "1574    349           can\n",
       "1016    358           are\n",
       "4662    361           now\n",
       "4743    390            on\n",
       "3235    416          have\n",
       "1552    443          call\n",
       "6539    453          that\n",
       "4704    460            of\n",
       "7424    508          your\n",
       "2821    518           for\n",
       "4489    550            my\n",
       "3623    568            it\n",
       "4238    601            me\n",
       "3612    679            is\n",
       "3502    683            in\n",
       "929     717           and\n",
       "6542   1004           the\n",
       "7420   1660           you\n",
       "6656   1670            to\n",
       "\n",
       "[7456 rows x 2 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame of tokens with their counts\n",
    "train_token_counts = pd.DataFrame({'token':train_features, 'count':np.sum(train_arr, axis=0)})\n",
    "train_token_counts.sort(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## MODEL BUILDING WITH NAIVE BAYES\n",
    "## http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "# train a Naive Bayes model using train_dtm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_dtm, y_train)\n",
    "    # note that we're fitting on train_dtm, because we need numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make predictions on test data using test_dtm\n",
    "y_pred_class = nb.predict(test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.988513998564\n",
      "[[1203    5]\n",
      " [  11  174]]\n"
     ]
    }
   ],
   "source": [
    "# compare predictions to true labels\n",
    "from sklearn import metrics\n",
    "print metrics.accuracy_score(y_test, y_pred_class)\n",
    "print metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.87744864e-03,   1.83488846e-05,   2.07301295e-03, ...,\n",
       "         1.09026171e-06,   1.00000000e+00,   3.98279868e-09])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict (poorly calibrated) probabilities and calculate AUC\n",
    "# can't take them seriously as calibrated\n",
    "y_pred_prob = nb.predict_proba(test_dtm)[:, 1]\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.986643100054\n"
     ]
    }
   ],
   "source": [
    "print metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Waiting for your call.', 'Also andros ice etc etc',\n",
       "       'No calls..messages..missed calls', 'No pic. Please re-send.',\n",
       "       'No calls..messages..missed calls'], dtype=object)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the message text for the false positives\n",
    "X_test[y_test < y_pred_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ \"LookAtMe!: Thanks for your purchase of a video clip from LookAtMe!, you've been charged 35p. Think you can do better? Why not send a video in a MMSto 32323.\",\n",
       "       \"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, \\xc2\\xa31.50 to rcv\",\n",
       "       \"Xmas & New Years Eve tickets are now on sale from the club, during the day from 10am till 8pm, and on Thurs, Fri & Sat night this week. They're selling fast!\",\n",
       "       \"Hi I'm sue. I am 20 years old and work as a lapdancer. I love sex. Text me live - I'm i my bedroom now. text SUE to 89555. By TextOperator G2 1DA 150ppmsg 18+\",\n",
       "       'Would you like to see my XXX pics they are so hot they were nearly banned in the uk!',\n",
       "       'CALL 09090900040 & LISTEN TO EXTREME DIRTY LIVE CHAT GOING ON IN THE OFFICE RIGHT NOW TOTAL PRIVACY NO ONE KNOWS YOUR [sic] LISTENING 60P MIN 24/7MP 0870753331018+',\n",
       "       'thesmszone.com lets you send free anonymous and masked messages..im sending this message from there..do you see the potential for abuse???',\n",
       "       'Hi this is Amy, we will be sending you a free phone number in a couple of days, which will give you an access to all the adult parties...',\n",
       "       \"INTERFLORA - \\xc2\\x93It's not too late to order Interflora flowers for christmas call 0800 505060 to place your order before Midnight tomorrow.\",\n",
       "       \"Hi ya babe x u 4goten bout me?' scammers getting smart..Though this is a regular vodafone no, if you respond you get further prem rate msg/subscription. Other nos used also. Beware!\",\n",
       "       'Money i have won wining number 946 wot do i do next'], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the message text for the false negatives\n",
    "X_test[y_test > y_pred_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###We can still use logistic regression###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## COMPARING NAIVE BAYES WITH LOGISTIC REGRESSION\n",
    "\n",
    "# instantiate/fit/predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(test_dtm)\n",
    "y_pred_prob = logreg.predict_proba(test_dtm)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.989231873654\n",
      "[[1206    2]\n",
      " [  13  172]]\n",
      "0.994144889923\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "print metrics.accuracy_score(y_test, y_pred_class)\n",
    "print metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey so it actually did a tiny bit better than Naive Bayes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'Cheers for the message Zogtorius. I\\xc2\\x92ve been staring at my phone for an age deciding whether to text or not.',\n",
       "       \"Forgot you were working today! Wanna chat, but things are ok so drop me a text when you're free / bored etc and i'll ring. Hope all is well, nose essay and all xx\"], dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show false positives and false negatives\n",
    "X_test[y_test < y_pred_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Call FREEPHONE 0800 542 0578 now!',\n",
       "       'Urgent Ur \\xc2\\xa3500 guaranteed award is still unclaimed! Call 09066368327 NOW closingdate04/09/02 claimcode M39M51 \\xc2\\xa31.50pmmorefrommobile2Bremoved-MobyPOBox734LS27YF',\n",
       "       \"LookAtMe!: Thanks for your purchase of a video clip from LookAtMe!, you've been charged 35p. Think you can do better? Why not send a video in a MMSto 32323.\",\n",
       "       'Would you like to see my XXX pics they are so hot they were nearly banned in the uk!',\n",
       "       'CALL 09090900040 & LISTEN TO EXTREME DIRTY LIVE CHAT GOING ON IN THE OFFICE RIGHT NOW TOTAL PRIVACY NO ONE KNOWS YOUR [sic] LISTENING 60P MIN 24/7MP 0870753331018+',\n",
       "       'thesmszone.com lets you send free anonymous and masked messages..im sending this message from there..do you see the potential for abuse???',\n",
       "       'RECPT 1/3. You have ordered a Ringtone. Your order is being processed...',\n",
       "       'Hi this is Amy, we will be sending you a free phone number in a couple of days, which will give you an access to all the adult parties...',\n",
       "       'Romantic Paris. 2 nights, 2 flights from \\xc2\\xa379 Book now 4 next year. Call 08704439680Ts&Cs apply.',\n",
       "       'England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/\\xc3\\xba1.20 POBOXox36504W45WQ 16+',\n",
       "       \"INTERFLORA - \\xc2\\x93It's not too late to order Interflora flowers for christmas call 0800 505060 to place your order before Midnight tomorrow.\",\n",
       "       \"Hi ya babe x u 4goten bout me?' scammers getting smart..Though this is a regular vodafone no, if you respond you get further prem rate msg/subscription. Other nos used also. Beware!\",\n",
       "       'Money i have won wining number 946 wot do i do next'], dtype=object)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[y_test > y_pred_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>token</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3684</th>\n",
       "      <td>319</td>\n",
       "      <td>1</td>\n",
       "      <td>gt</td>\n",
       "      <td>0.003135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4793</th>\n",
       "      <td>317</td>\n",
       "      <td>1</td>\n",
       "      <td>lt</td>\n",
       "      <td>0.003155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3805</th>\n",
       "      <td>232</td>\n",
       "      <td>1</td>\n",
       "      <td>he</td>\n",
       "      <td>0.004310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6843</th>\n",
       "      <td>168</td>\n",
       "      <td>1</td>\n",
       "      <td>she</td>\n",
       "      <td>0.005952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>lor</td>\n",
       "      <td>0.006135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2428</th>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>da</td>\n",
       "      <td>0.006623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4550</th>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>later</td>\n",
       "      <td>0.007353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>ask</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6626</th>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>said</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2714</th>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>doing</td>\n",
       "      <td>0.011236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>amp</td>\n",
       "      <td>0.011236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>morning</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>231</td>\n",
       "      <td>3</td>\n",
       "      <td>come</td>\n",
       "      <td>0.012987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>anything</td>\n",
       "      <td>0.012987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>cos</td>\n",
       "      <td>0.012987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4724</th>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>lol</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7463</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>sure</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7099</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>something</td>\n",
       "      <td>0.014286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3690</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>gud</td>\n",
       "      <td>0.014706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3171</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>feel</td>\n",
       "      <td>0.015873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8394</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>went</td>\n",
       "      <td>0.015873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5371</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>nice</td>\n",
       "      <td>0.015873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>gonna</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7001</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>sleep</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>always</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5254</th>\n",
       "      <td>755</td>\n",
       "      <td>13</td>\n",
       "      <td>my</td>\n",
       "      <td>0.017219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5217</th>\n",
       "      <td>116</td>\n",
       "      <td>2</td>\n",
       "      <td>much</td>\n",
       "      <td>0.017241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5533</th>\n",
       "      <td>115</td>\n",
       "      <td>2</td>\n",
       "      <td>oh</td>\n",
       "      <td>0.017391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2815</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>dun</td>\n",
       "      <td>0.017857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3925</th>\n",
       "      <td>166</td>\n",
       "      <td>3</td>\n",
       "      <td>home</td>\n",
       "      <td>0.018072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>bonus</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3994</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>http</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6619</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>sae</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>8007</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>800</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5297</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>national</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8375</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>weekly</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8153</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>valid</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>10p</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>5000</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5117</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>mob</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>16</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>collection</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7838</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>tones</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2963</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>entry</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596</th>\n",
       "      <td>3</td>\n",
       "      <td>99</td>\n",
       "      <td>www</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6525</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>ringtone</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>150ppm</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8016</th>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>uk</td>\n",
       "      <td>37.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>awarded</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>1000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>500</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>cs</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3688</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>guaranteed</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>18</td>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7837</th>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>tone</td>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>150p</td>\n",
       "      <td>72.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6113</th>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>prize</td>\n",
       "      <td>94.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>1</td>\n",
       "      <td>114</td>\n",
       "      <td>claim</td>\n",
       "      <td>114.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8713 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ham  spam       token  spam_ratio\n",
       "3684  319     1          gt    0.003135\n",
       "4793  317     1          lt    0.003155\n",
       "3805  232     1          he    0.004310\n",
       "6843  168     1         she    0.005952\n",
       "4747  163     1         lor    0.006135\n",
       "2428  151     1          da    0.006623\n",
       "4550  136     1       later    0.007353\n",
       "1247   90     1         ask    0.011111\n",
       "6626   90     1        said    0.011111\n",
       "2714   89     1       doing    0.011236\n",
       "1084   89     1         amp    0.011236\n",
       "5167   80     1     morning    0.012500\n",
       "2163  231     3        come    0.012987\n",
       "1142   77     1    anything    0.012987\n",
       "2289   77     1         cos    0.012987\n",
       "4724   75     1         lol    0.013333\n",
       "7463   72     1        sure    0.013889\n",
       "7099   70     1   something    0.014286\n",
       "3690   68     1         gud    0.014706\n",
       "3171   63     1        feel    0.015873\n",
       "8394   63     1        went    0.015873\n",
       "5371   63     1        nice    0.015873\n",
       "3595   59     1       gonna    0.016949\n",
       "7001   59     1       sleep    0.016949\n",
       "1064   59     1      always    0.016949\n",
       "5254  755    13          my    0.017219\n",
       "5217  116     2        much    0.017241\n",
       "5533  115     2          oh    0.017391\n",
       "2815   56     1         dun    0.017857\n",
       "3925  166     3        home    0.018072\n",
       "...   ...   ...         ...         ...\n",
       "1623    1    22       bonus   22.000000\n",
       "3994    1    22        http   22.000000\n",
       "6619    1    22         sae   22.000000\n",
       "735     1    22        8007   22.000000\n",
       "732     1    23         800   23.000000\n",
       "5297    1    23    national   23.000000\n",
       "8375    1    25      weekly   25.000000\n",
       "8153    1    25       valid   25.000000\n",
       "309     1    25         10p   25.000000\n",
       "618     1    26        5000   26.000000\n",
       "5117    1    26         mob   26.000000\n",
       "364     2    54          16   27.000000\n",
       "2150    1    27  collection   27.000000\n",
       "7838    1    27       tones   27.000000\n",
       "2963    1    27       entry   27.000000\n",
       "1       1    30         000   30.000000\n",
       "8596    3    99         www   33.000000\n",
       "6525    1    33    ringtone   33.000000\n",
       "356     1    35      150ppm   35.000000\n",
       "8016    2    75          uk   37.500000\n",
       "1333    1    39     awarded   39.000000\n",
       "299     1    42        1000   42.000000\n",
       "617     1    45         500   45.000000\n",
       "2371    1    45          cs   45.000000\n",
       "3688    1    51  guaranteed   51.000000\n",
       "369     1    52          18   52.000000\n",
       "7837    1    61        tone   61.000000\n",
       "352     1    72        150p   72.000000\n",
       "6113    1    94       prize   94.000000\n",
       "2067    1   114       claim  114.000000\n",
       "\n",
       "[8713 rows x 4 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## BONUS CONTENT: CALCULATING THE 'SPAMMINESS' OF EACH TOKEN\n",
    "\n",
    "# create separate DataFrames for ham and spam\n",
    "sms_ham = sms[sms.label==0]\n",
    "sms_spam = sms[sms.label==1]\n",
    "\n",
    "# learn the vocabulary of ALL messages and save it\n",
    "vect.fit(sms.msg)\n",
    "all_features = vect.get_feature_names()\n",
    "\n",
    "# create document-term matrix of ham, then convert to a regular array\n",
    "ham_dtm = vect.transform(sms_ham.msg)\n",
    "ham_arr = ham_dtm.toarray()\n",
    "\n",
    "# create document-term matrix of spam, then convert to a regular array\n",
    "spam_dtm = vect.transform(sms_spam.msg)\n",
    "spam_arr = spam_dtm.toarray()\n",
    "\n",
    "# count how many times EACH token appears across ALL messages in ham_arr\n",
    "ham_counts = np.sum(ham_arr, axis=0)\n",
    "\n",
    "# count how many times EACH token appears across ALL messages in spam_arr\n",
    "spam_counts = np.sum(spam_arr, axis=0)\n",
    "\n",
    "# create a DataFrame of tokens with their separate ham and spam counts\n",
    "all_token_counts = pd.DataFrame({'token':all_features, 'ham':ham_counts, 'spam':spam_counts})\n",
    "\n",
    "# add one to ham counts and spam counts so that ratio calculations (below) make more sense\n",
    "all_token_counts['ham'] = all_token_counts.ham + 1\n",
    "all_token_counts['spam'] = all_token_counts.spam + 1\n",
    "\n",
    "# calculate ratio of spam-to-ham for each token\n",
    "all_token_counts['spam_ratio'] = all_token_counts.spam / all_token_counts.ham\n",
    "all_token_counts.sort('spam_ratio')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
